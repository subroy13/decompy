<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="11" skipped="0" tests="51" time="1.203" timestamp="2024-03-10T12:50:01.817776" hostname="subroy13PC"><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_singular_values_as_matrix" time="0.003" /><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_singular_values_as_array" time="0.001" /><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_singular_vectors_left" time="0.001" /><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_singular_vectors_right" time="0.001" /><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_singular_vectors_both" time="0.001" /><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_convergence_metrics" time="0.001" /><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_cumulative_variance_identity" time="0.001" /><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_cumulative_variance_proportion" time="0.001" /><testcase classname="tests.test_interfaces.TestSVDResultInterface" name="test_estimated_rank" time="0.001" /><testcase classname="tests.test_interfaces.TestPCAResultInterface" name="test_location" time="0.001" /><testcase classname="tests.test_interfaces.TestPCAResultInterface" name="test_eigen_values_as_matrix" time="0.001" /><testcase classname="tests.test_interfaces.TestPCAResultInterface" name="test_eigen_values_as_array" time="0.001" /><testcase classname="tests.test_interfaces.TestPCAResultInterface" name="test_eigen_vectors" time="0.001" /><testcase classname="tests.test_interfaces.TestPCAResultInterface" name="test_convergence_metrics" time="0.001" /><testcase classname="tests.test_interfaces.TestPCAResultInterface" name="test_cumulative_variance_identity" time="0.001" /><testcase classname="tests.test_interfaces.TestPCAResultInterface" name="test_cumulative_variance_proportion" time="0.001" /><testcase classname="tests.test_interfaces.TestPCAResultInterface" name="test_estimated_rank" time="0.001" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_singular_values_as_matrix" time="0.001" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_singular_values_as_array" time="0.001" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_singular_vectors_left" time="0.002" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_singular_vectors_right" time="0.001" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_singular_vectors_both" time="0.001" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_convergence_metrics" time="0.001" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_cumulative_variance_identity" time="0.001" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_cumulative_variance_proportion" time="0.001" /><testcase classname="tests.test_interfaces.TestLSNResultInterface" name="test_estimated_rank" time="0.001" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_singular_values_as_matrix" time="0.002" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_singular_values_as_array" time="0.001" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_singular_vectors_left" time="0.002" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_singular_vectors_right" time="0.001" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_singular_vectors_both" time="0.001" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_convergence_metrics" time="0.001" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_cumulative_variance_identity" time="0.001" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_cumulative_variance_proportion" time="0.001" /><testcase classname="tests.test_interfaces.TestRankFactorizationResultInterface" name="test_estimated_rank" time="0.001" /><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_adm" time="0.002" /><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_alm" time="0.001"><failure message="ValueError: attempt to get argmax of an empty sequence">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C46860&gt;
sample_matrix = array([[0.72038009, 0.48468797, 0.72188598, 0.09092241],
       [0.70711431, 0.67604674, 0.07257596, 0.70115495],
    ...91],
       [0.41731765, 0.31779727, 0.34670363, 0.208563  ],
       [0.58693177, 0.89051221, 0.59065247, 0.71371125]])

    def test_alm(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = AugmentedLagrangianMethod()
&gt;       res = mod.decompose(X, rank=min(n, p))

tests\test_matrix_factorization.py:69: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\decompy\matrix_factorization\alm.py:136: in decompose
    max_idx = np.argmax(ratio)
C:\Users\roysu\AppData\Roaming\Python\Python310\site-packages\numpy\core\fromnumeric.py:1229: in argmax
    return _wrapfunc(a, 'argmax', axis=axis, out=out, **kwds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = array([], dtype=float64), method = 'argmax', args = (), kwds = {'axis': None, 'out': None}
bound = &lt;built-in method argmax of numpy.ndarray object at 0x000001C990C5EFD0&gt;

    def _wrapfunc(obj, method, *args, **kwds):
        bound = getattr(obj, method, None)
        if bound is None:
            return _wrapit(obj, method, *args, **kwds)
    
        try:
&gt;           return bound(*args, **kwds)
E           ValueError: attempt to get argmax of an empty sequence

C:\Users\roysu\AppData\Roaming\Python\Python310\site-packages\numpy\core\fromnumeric.py:59: ValueError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_asrpca" time="0.001"><failure message="ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 5)">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C46A10&gt;
sample_matrix = array([[0.66211811, 0.62267918, 0.61950562, 0.53578402],
       [0.99082353, 0.05770659, 0.64399173, 0.55659965],
    ...16],
       [0.88150177, 0.60089291, 0.35911937, 0.63197344],
       [0.06280921, 0.01091304, 0.93934631, 0.24015159]])

    def test_asrpca(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = ActiveSubspaceRobustPCA()
&gt;       res = mod.decompose(X, k=min(n, p))

tests\test_matrix_factorization.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.asrpca.ActiveSubspaceRobustPCA object at 0x000001C990C960B0&gt;
M = array([[0.66211811, 0.62267918, 0.61950562, 0.53578402],
       [0.99082353, 0.05770659, 0.64399173, 0.55659965],
    ...16],
       [0.88150177, 0.60089291, 0.35911937, 0.63197344],
       [0.06280921, 0.01091304, 0.93934631, 0.24015159]])
k = 4, lambd = 0.5

    def decompose(self, M: np.ndarray, k: Union[int, None] = None, lambd=None):
        """Decompose a matrix into low rank and sparse components.
    
        Decomposes the input matrix `M` into the sum of
        a low-rank matrix `L` and a sparse matrix `S`,
        by solving the optimization problem:
    
        min |L|_* + lambda |S|_1
        s.t. M = L + S
    
        Parameters
        ----------
        M : ndarray
            Input matrix to decompose
    
        k : int or None, optional
            Rank of the low-rank component.
            If None, default is max(1, round(min(M.shape)/10))
    
        lambd : float, optional
            Regularization parameter for the sparsity term.
            If None, default is 1/sqrt(min(M.shape))
    
        Returns
        -------
        LSNResult
            Named tuple containing:
    
            L : Low-rank component
    
            S : Sparse component
    
            N : Null component (all zeros)
    
            convergence : Dictionary with keys:
                'niter' : Number of iterations
                'converged' : Whether converged within max iterations
                'final_error' : Final reconstruction error
    
        """
        check_real_matrix(M)
        X = M.copy()  # create a copy to avoid modifying true matrix
    
        d, n = X.shape
        if lambd is None:
            lambd = 1 / np.sqrt(min(d, n))
        if k is None:
            k = max(1, np.round(min(d, n) / 10))
    
        tol = self.tol * np.linalg.norm(X, "fro")
        mu = 1.0 / np.linalg.norm(X, 2)
    
        # initialize optimization variables
        J = np.zeros((k, n))
        E = np.zeros((d, n))
        Y = np.zeros((d, n))
    
        # start the main loop
        niter = 0
        while niter &lt; self.maxiter:
            niter += 1
            dey = X - E + Y / mu
            temp = dey @ J.T
    
            # update Q
            U, sigma, Vt = np.linalg.svd(temp)
&gt;           Q = U @ Vt
E           ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 5)

src\decompy\matrix_factorization\asrpca.py:103: ValueError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_dualrpca" time="0.030"><failure message="IndexError: index 142 is out of bounds for axis 0 with size 7">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C46BC0&gt;
sample_matrix = array([[0.85549004, 0.9369287 , 0.1739737 , 0.11996052],
       [0.86539685, 0.15668203, 0.37299316, 0.25610698],
    ...48],
       [0.37700556, 0.41548364, 0.7649564 , 0.49780942],
       [0.80917011, 0.42141234, 0.40134695, 0.00214426]])

    def test_dualrpca(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = DualRobustPCA()
&gt;       res = mod.decompose(X)

tests\test_matrix_factorization.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.dual.DualRobustPCA object at 0x000001C992D20E20&gt;
M = array([[0.85549004, 0.9369287 , 0.1739737 , 0.11996052],
       [0.86539685, 0.15668203, 0.37299316, 0.25610698],
    ...48],
       [0.37700556, 0.41548364, 0.7649564 , 0.49780942],
       [0.80917011, 0.42141234, 0.40134695, 0.00214426]])
lambd = 0.4472135954999579

    def decompose(self, M: np.ndarray, lambd: Union[float, None] = None):
        """Decompose a matrix M into two low-rank matrices using dual proximal gradient.
    
        Parameters
        ----------
        M : ndarray
            The input matrix to decompose.
        lambd : float or None, optional
            Regularization parameter. Default is 1/sqrt(m) where m is the number of rows of M.
    
        Returns
        -------
        LSNResult
            A named tuple containing the low rank matrix L,
            sparse matrix S, optional noise matrix N,
            and convergence info.
    
        """
        check_real_matrix(M)
        D = M.copy()  # this is copy so that we don't modify the thing
        m, n = D.shape
        if lambd is None:
            lambd = 1 / np.sqrt(m)
    
        # initialize
        Y = np.sign(D)
        norm_two = np.linalg.norm(Y, ord=2)
        norm_inf = np.linalg.norm(Y, ord=np.inf) / lambd
        dual_norm = max(norm_two, norm_inf)
        Y /= dual_norm
        norm_two /= dual_norm
        norm_inf /= dual_norm
    
        # projection
        A_dual = np.zeros((m, n))
        E_dual = np.zeros((m, n))
        tolProj = 1e-8 * np.linalg.norm(D, "fro")
    
        # linesearch
        t = 1
        K = 7
        delta = 0.1
        memo_step_size = np.ones(K) * 0.1
    
        niter = 0
        converged = False
        while not converged:
            # compute Z, projection of D onto the normal cone of Y
            # get the search direction D - Z
            niter += 1
    
            if norm_two &lt; norm_inf - self.eps and niter &lt; self.maxiter:
                threshold = np.linalg.norm(Y, np.inf) * (1 - self.eps_proj)
                Z = np.maximum(D * (Y &gt; threshold), 0) + np.minimum(
                    D * (Y &lt; -threshold), 0
                )
            else:
                t = max(np.round(t * 1.1), t + 1)
                u, s, vt = np.linalg.svd(Y)
                t = np.max(np.where(s &gt;= s[0] * (1 - 1e-2)))
    
                if norm_two &gt; norm_inf + self.eps and niter &lt; self.maxiter:
                    D_bar = u[:, :t].T @ D @ vt[:t, :].T
                    J, S = np.linalg.eig((D_bar + D_bar.T) / 2)
                    temp = S @ np.diag(np.maximum(J, 0)) @ S.T
                    Z = u[:, :t] @ temp @ vt[:t, :]
                else:
                    convergedProjection = False
                    A_dual = np.zeros((m, n))
                    E_dual = np.zeros((m, n))
                    proj = 0
                    threshold = np.linalg.norm(Y, np.inf) * (1 - self.eps_proj)
                    while not convergedProjection:
                        Z = D - A_dual
                        Z = np.maximum(Z * (Y &gt; threshold), 0) + np.minimum(
                            Z * (Y &lt; -threshold), 0
                        )
                        D_bar = u[:, :t].T @ (D - Z) @ vt[:t, :].T
                        J, S = np.linalg.eig((D_bar + D_bar.T) / 2)
                        temp = S @ np.diag(np.maximum(J, 0)) @ S.T
                        X = u[:, :t] @ temp @ vt[:t, :]
    
                        if (
                            np.linalg.norm(Z - E_dual, "fro") &lt; tolProj
                            and np.linalg.norm(X - A_dual, "fro") &lt; tolProj
                        ):
                            convergedProjection = True
                            E_dual, A_dual = Z, X
                            Z += X
                        else:
                            E_dual, A_dual = Z, X
    
                        proj += 1
                        if proj &gt; 50:
                            # max inner iteration reached
                            convergedProjection = True
    
                # linesearch to find max trace(D'*(Y+delta*(D-Z)))/J(Y+delta*(D-Z))
                Z = D - Z
                a = np.dot(D.reshape(-1), Y.reshape(-1))
                b = np.dot(D.reshape(-1), Z.reshape(-1))
                c = np.dot(Z.reshape(-1), Z.reshape(-1))
    
                if not self.linesearch:
    
                    # non exact linesearch
                    stepsize = max(1.3 * np.median(memo_step_size), 1e-4)
                    converged_line_search_like = False
                    num_trial_point = 0
                    while not converged_line_search_like:
                        X = Y + stepsize * Z
                        norm_two = np.linalg.norm(X, 2)
                        norm_inf = np.linalg.norm(X, np.inf) / lambd
                        dual_norm = max(norm_two, norm_inf)
                        tempv = (a + b * stepsize) / dual_norm
                        diff = tempv - a - stepsize / 2 * c
                        if diff &gt; 0 or num_trial_point &gt;= 50:
                            converged_line_search_like = True
                            norm_two /= dual_norm
                            norm_inf /= dual_norm
                            Y = X / dual_norm
                            delta = stepsize
                        else:
                            stepsize *= self.beta
                        num_trial_point += 1
&gt;                   memo_step_size[int(niter / K)] = delta
E                   IndexError: index 142 is out of bounds for axis 0 with size 7

src\decompy\matrix_factorization\dual.py:205: IndexError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_ealm" time="0.001"><failure message="ValueError: operands could not be broadcast together with shapes (5,0) (0,0)">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C46D70&gt;
sample_matrix = array([[0.46744965, 0.11840755, 0.85064279, 0.22651523],
       [0.90184216, 0.32494294, 0.80490853, 0.9336687 ],
    ...72],
       [0.98985182, 0.81728252, 0.99225385, 0.10232704],
       [0.6878782 , 0.55314868, 0.7561408 , 0.55244717]])

    def test_ealm(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = ExactAugmentedLagrangianMethod()
&gt;       res = mod.decompose(X)

tests\test_matrix_factorization.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.ealm.ExactAugmentedLagrangianMethod object at 0x000001C990C975E0&gt;
M = array([[0.46744965, 0.11840755, 0.85064279, 0.22651523],
       [0.90184216, 0.32494294, 0.80490853, 0.9336687 ],
    ...72],
       [0.98985182, 0.81728252, 0.99225385, 0.10232704],
       [0.6878782 , 0.55314868, 0.7561408 , 0.55244717]])
lambd = 0.4472135954999579, mu = 0.11180339887498948, rho = 6

    def decompose(
        self, M: np.ndarray, lambd: float = None, mu: float = None, rho: float = 6
    ):
        """Decompose a matrix M into a low-rank matrix L and a sparse matrix S.
    
        Parameters
        ----------
        M : ndarray
            Input matrix to decompose
    
        lambd : float, optional
            Weight on sparse error term in cost function.
            Default is 1/sqrt(m) where m is number of rows in M
    
        mu : float, optional
            Regularization parameter for low-rank matrix.
            Default is 0.5 / norm_two where norm_two is L2 norm of M
    
        rho : float, default 6
            Parameter for increasing mu at each iteration
    
        Returns
        -------
        LSNResult
            Named tuple containing low-rank matrix L, sparse matrix S
            and convergence info
        """
    
        check_real_matrix(M)
        D = M.copy()
        m, n = D.shape
        Y = np.sign(D)
    
        if lambd is None:
            lambd = 1 / np.sqrt(m)
    
        norm_two = np.linalg.norm(Y, 2)
        norm_inf = np.linalg.norm(Y, np.Inf) / lambd
        dual_norm = max(norm_two, norm_inf)
        Y /= dual_norm
    
        A_hat = np.zeros_like(D)
        E_hat = np.zeros_like(D)
        dnorm = np.linalg.norm(D, "fro")
        tol_proj = 1e-6 * dnorm
        total_svd = 0
        if mu is None:
            mu = 0.5 / norm_two
    
        iter = 0
        converged = False
        sv = 5
        svp = sv
        while not converged:
            iter += 1
    
            # solve the primal problem by alternative projection
            primal_converged = False
            primal_iter = 0
            sv += np.round(n * 0.1)
            while not primal_converged:
                temp_T = D - A_hat + (1 / mu) * Y
                temp_E = np.maximum(temp_T - lambd / mu, 0) + np.minimum(
                    temp_T + lambd / mu, 0
                )
                U, diagS, VT = np.linalg.svd(
                    D - temp_E + (1 / mu) * Y, full_matrices=False
                )
                svp = (diagS &gt; 1 / mu).sum()
                sv = min(svp + 1, n) if svp &lt; sv else min(svp + np.round(0.05 * n), n)
&gt;               temp_A = U[:, :svp] * np.diag(diagS[:svp] - 1 / mu) * VT[:svp, :]
E               ValueError: operands could not be broadcast together with shapes (5,0) (0,0)

src\decompy\matrix_factorization\ealm.py:107: ValueError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_fpcp" time="0.002"><failure message="IndexError: index 4 is out of bounds for axis 0 with size 4">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C46F20&gt;
sample_matrix = array([[0.78955757, 0.81274655, 0.82488546, 0.51856732],
       [0.49679959, 0.53927506, 0.78132655, 0.46933711],
    ...12],
       [0.65190972, 0.11069995, 0.87095352, 0.87416223],
       [0.58073755, 0.03926434, 0.9020521 , 0.83963167]])

    def test_fpcp(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = FastPrincipalComponentPursuit()
&gt;       res = mod.decompose(X, initrank=min(n, p))

tests\test_matrix_factorization.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.fpcp.FastPrincipalComponentPursuit object at 0x000001C992D2F940&gt;
M = array([[0.78955757, 0.81274655, 0.82488546, 0.51856732],
       [0.49679959, 0.53927506, 0.78132655, 0.46933711],
    ...12],
       [0.65190972, 0.11069995, 0.87095352, 0.87416223],
       [0.58073755, 0.03926434, 0.9020521 , 0.83963167]])
initrank = 4, rank_threshold = 0.01, lambdaval = 0.4472135954999579, lambdafactor = 1

    def decompose(
        self,
        M: np.ndarray,
        initrank: Union[int, None] = None,
        rank_threshold: Union[float, None] = None,
        lambdaval: Union[float, None] = None,
        lambdafactor: Union[float, None] = None,
    ):
        """Decompose a matrix M using FPCP method.
    
        Parameters
        ----------
        M : ndarray
            The input matrix to decompose.
        initrank : int or None, optional
            The initial rank estimate. If None, set to 1.
            Default is None.
        rank_threshold : float or None, optional
            The threshold value for incrementing the rank.
            If None, set to 0.01.
            Default is None.
        lambdaval : float or None, optional
            The regularization parameter. If None, set to 1/sqrt(max(n,p)).
            Default is None.
        lambdafactor : float or None, optional
            The factor to decrease lambda at each iteration.
            If None, set to 1.
            Default is None.
    
        Returns
        -------
        SVDResult
            A named tuple containing the final U, S, V^T matrices along
            with convergence information.
        """
        check_real_matrix(M)
        X = np.copy(M)  # to ensure that original matrix is not modified
        n, p = X.shape
        lambdaval = lambdaval if lambdaval is not None else 1 / np.sqrt(max(n, p))
        lambdafactor = 1 if lambdafactor is None else lambdafactor
        rank0 = 1 if initrank is None else initrank
        rank_threshold = 0.01 if rank_threshold is None else rank_threshold
    
        inc_rank = True  # a flag to increment the rank plus one at each iteration
    
        # First outer loop
        rank = rank0
        Ulan, Slan, Vtlan = np.linalg.svd(X)
        Ulan = Ulan[:, :rank]
        Slan = Slan[:rank]
        Vtlan = Vtlan[:rank, :]
    
        # current low rank approximation
        L1 = Ulan @ np.diag(Slan) @ Vtlan
    
        # shrinkage
        S1 = self._shrink(X - L1, lambdaval)
    
        # Outer loops
        niter = 0
        while True:
            niter += 1
            if inc_rank:
                lambdaval *= lambdafactor
                rank += 1
    
            # get the current rank estimate
            Ulan, Slan, Vtlan = np.linalg.svd(X - S1)
            Ulan = Ulan[:, :rank]
            Slan = Slan[:rank]
            Vtlan = Vtlan[:rank, :]
    
            # simple rule to keep or increase the current rank's value
&gt;           rho = Slan[rank - 1] / np.sum(Slan[: (rank - 1)])
E           IndexError: index 4 is out of bounds for axis 0 with size 4

src\decompy\matrix_factorization\fpcp.py:125: IndexError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_ga" time="0.002"><failure message="ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 5)">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C470D0&gt;
sample_matrix = array([[0.3468674 , 0.25094957, 0.88171855, 0.17376655],
       [0.24930488, 0.89619251, 0.60054576, 0.48946979],
    ...4 ],
       [0.31999703, 0.73986767, 0.39732558, 0.58706586],
       [0.73787388, 0.33396649, 0.25697332, 0.87148554]])

    def test_ga(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = GrassmannAverage()
&gt;       res = mod.decompose(X, min(n, p))

tests\test_matrix_factorization.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.ga.GrassmannAverage object at 0x000001C990C969E0&gt;
M = array([[0.3468674 , 0.25094957, 0.88171855, 0.17376655],
       [0.24930488, 0.89619251, 0.60054576, 0.48946979],
    ...4 ],
       [0.31999703, 0.73986767, 0.39732558, 0.58706586],
       [0.73787388, 0.33396649, 0.25697332, 0.87148554]])
K = 4

    def decompose(self, M: np.ndarray, K: Union[int, None] = None):
        """Decompose a matrix M into two low rank matrices using Grassmann averages.
    
        Parameters
        ----------
        M : ndarray
            The input matrix to decompose, of shape (N, D).
    
        K : int or None, optional
            The target rank for decomposition. If not provided, default is 1.
    
        Returns
        -------
        res : RankFactorizationResult
            A named tuple containing the low rank factors A and B, as well as
            convergence diagnostics.
    
        Notes
        -----
        The algorithm is based on Grassmann averages. It iteratively extracts
        the top K principal components while orthogonalizing them.
    
        Examples
        --------
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; from decompy.matrix_factorization import GrassmannAverage
        &gt;&gt;&gt; M = np.random.rand(10, 20)
        &gt;&gt;&gt; ga = GrassmannAverage()
        &gt;&gt;&gt; res = ga.decompose(M, K=5)
        &gt;&gt;&gt; A = res.A
        &gt;&gt;&gt; B = res.B
        """
        check_real_matrix(M)
        X = M.copy()  # create a copy of the matrix to avoid side effects
        n, d = X.shape
        if K is None:
            K = 1
        if K &gt; d:
            K = d
        vectors = np.zeros((d, K))
    
        converged = np.zeros(K)  # convergence metrics
        niter = np.zeros(K)
    
        for k in range(K):
            # compute the k-th principal component
            mu = np.random.random(size=d).reshape(-1) - 0.5
            mu = mu / np.linalg.norm(mu)
    
            # initialize using a few EM iterations
            for _ in range(self.em_iter):
                dots = X @ mu  # (N,)
                mu = X.T @ dots  # (D,)
                mu /= np.linalg.norm(mu)
    
            # now the grassmann average
            for iter in range(n):
                prev_mu = mu
    
                # compute angles and flip
                dot_signs = np.sign(X @ mu)  # (N, )
    
                # compute weighted grassmann mean / trimmed mean
                if self.trim_percent &gt; 0:
                    weighted_product = (
                        X * dot_signs[:, np.newaxis]
                    )  # Element-wise multiplication with broadcasting
                    mu = np.mean(
                        np.percentile(weighted_product, self.trim_percent, axis=0),
                        axis=0,
                    )  # Compute trimmed mean
                else:
                    mu = X.T @ dot_signs  # (D, 1)
                mu = mu.reshape(-1)  # (D, )
                mu /= np.linalg.norm(mu)
    
                # check for convergence
                if np.max(np.abs(mu - prev_mu)) &lt; self.eps:
                    break
            converged[k] = iter &lt; n  # if max iteration is not reached, means converged
            niter[k] = iter
    
            # store the estimated vector
            # and possibly subtract it from data, and perform reorthonomralisation
            if k == 0:
                vectors[:, k] = mu
&gt;               X -= X @ mu @ mu.T
E               ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 5)

src\decompy\matrix_factorization\ga.py:229: ValueError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_ialm" time="0.002"><failure message="ValueError: operands could not be broadcast together with shapes (5,0) (0,0)">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C46F50&gt;
sample_matrix = array([[0.25903956, 0.4770545 , 0.48340183, 0.92582218],
       [0.33046881, 0.23523665, 0.83368112, 0.37154431],
    ...57],
       [0.25409834, 0.98937691, 0.90297391, 0.19342111],
       [0.36214135, 0.33133854, 0.66447004, 0.05782536]])

    def test_ialm(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = InexactAugmentedLagrangianMethod()
&gt;       res = mod.decompose(X)

tests\test_matrix_factorization.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.ialm.InexactAugmentedLagrangianMethod object at 0x000001C992D2FA00&gt;
M = array([[0.25903956, 0.4770545 , 0.48340183, 0.92582218],
       [0.33046881, 0.23523665, 0.83368112, 0.37154431],
    ...57],
       [0.25409834, 0.98937691, 0.90297391, 0.19342111],
       [0.36214135, 0.33133854, 0.66447004, 0.05782536]])
lambd = 0.4472135954999579, mu = 0.537564882705047, rho = 1.5

    def decompose(
        self, M: np.ndarray, lambd: float = None, mu: float = None, rho: float = 1.5
    ):
        """Decompose a matrix M into a low rank L and sparse S using Inexact ALM.
    
        Parameters
        ----------
        M : ndarray
            Input matrix to decompose
        lambd : float, optional
            Weight on sparse error term in cost function.
            Default is 1/sqrt(m) where m is number of rows in M
        mu : float, optional
            Initial penalty parameter.
            Default is 1.25 / frobenius norm of M
        rho : float, optional
            Factor to increase mu by at each iteration.
            Default is 1.5
    
        Returns
        -------
        LSNResult
            Named tuple containing low rank L, sparse S, and convergence info.
    
        Notes
        -----
        The Inexact ALM algorithm decomposes M into L and S by
        minimizing the objective:
    
            ||L||_* + lambd||S||_1
            s.t. M = L + S
    
        where ||.||_* is nuclear norm (sum of singular values) and
        ||.||_1 is L1 norm (sum of absolute values).
    
        """
        check_real_matrix(M)
        D = M.copy()
        m, n = D.shape
        if lambd is None:
            lambd = 1 / np.sqrt(m)
    
        # initialize
        Y = D
        norm_two = np.linalg.norm(Y, 2)
        norm_inf = np.linalg.norm(Y, np.Inf) / lambd
        dual_norm = max(norm_two, norm_inf)
        Y /= dual_norm
    
        A_hat = np.zeros_like(D)
        E_hat = np.zeros_like(D)
        if mu is None:
            mu = 1.25 / norm_two
        mu_bar = mu * 1e7
        dnorm = np.linalg.norm(D, "fro")
    
        iter = 0
        total_svd = 0
        converged = False
        sv = 10
    
        # main iteration loop
        while not converged:
            iter += 1
    
            temp_T = D - A_hat + (1 / mu) * Y
            E_hat = np.maximum(temp_T - lambd / mu, 0) + np.minimum(
                temp_T + lambd / mu, 0
            )
    
            U, diagS, VT = np.linalg.svd(D - E_hat + (1 / mu) * Y, full_matrices=False)
            svp = (diagS &gt; 1 / mu).sum()
            sv = min(svp + 1, n) if svp &lt; sv else min(svp + np.round(0.05 * n), n)
&gt;           A_hat = U[:, :svp] * np.diag(diagS[:svp] - 1 / mu) * VT[:svp, :]
E           ValueError: operands could not be broadcast together with shapes (5,0) (0,0)

src\decompy\matrix_factorization\ialm.py:111: ValueError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_l1f" time="0.001"><failure message="TypeError: slice indices must be integers or None or have an __index__ method">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C46A40&gt;
sample_matrix = array([[0.9090497 , 0.47717821, 0.15038356, 0.27972178],
       [0.97924182, 0.34002923, 0.70853607, 0.57476282],
    ...94],
       [0.18905359, 0.98969954, 0.67692075, 0.00280266],
       [0.44490061, 0.16041086, 0.05562631, 0.64084326]])

    def test_l1f(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = L1Filtering()
&gt;       res = mod.decompose(X)

tests\test_matrix_factorization.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\decompy\matrix_factorization\l1f.py:222: in decompose
    (A_seed, column_seed, row_seed, rA) = self._generate_seed(D, sr, sc)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.l1f.L1Filtering object at 0x000001C990CE6440&gt;
D = array([[0.9090497 , 0.47717821, 0.15038356, 0.27972178],
       [0.97924182, 0.34002923, 0.70853607, 0.57476282],
    ...94],
       [0.18905359, 0.98969954, 0.67692075, 0.00280266],
       [0.44490061, 0.16041086, 0.05562631, 0.64084326]])
sr = 1, sc = 1

    def _generate_seed(self, D: np.ndarray, sr, sc):
        """Generate seeds i.e., initialization values for the L1 filtering
    
        Parameters
        ----------
        D : numpy.ndarray
            The input matrix
        sr : int
            Desired number of rows in seed matrix
        sc : int
            Desired number of columns in seed matrix
    
        Returns
        -------
        A_seed : numpy.ndarray
            Seeded low-rank approximation of D
        column_seed : numpy.ndarray
            Column indices for seed matrix
        row_seed : numpy.ndarray
            Row indices for seed matrix
        rA : int
            Rank of the seed matrix
    
        Generates a low rank seed matrix for the input matrix D by randomly
        sampling rows and columns. The number of rows and columns sampled
        is determined by the desired size sr and sc. Singular value
        decomposition is used to find the approximate rank rA of the
        sampled matrix, and more rows/columns are sampled if needed to
        achieve the desired size sr * rA and sc * rA. The seeded low rank
        approximation A_seed is returned along with the sampled row and column
        indices.
        """
        if sr is None:
            sr = 10
        if sc is None:
            sc = 10
        thres = 1e-3
    
        m, n = D.shape
    
        dr = np.ceil(m / sr)
        dc = np.ceil(n / sc)
    
&gt;       row_seed = np.random.permutation(m)[:dr]
E       TypeError: slice indices must be integers or None or have an __index__ method

src\decompy\matrix_factorization\l1f.py:79: TypeError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_ladmap" time="0.003" /><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_mest" time="0.002"><failure message="TypeError: slice indices must be integers or None or have an __index__ method">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C473A0&gt;
sample_matrix = array([[0.70735595, 0.42514445, 0.33489327, 0.8183266 ],
       [0.99983977, 0.16107967, 0.65322436, 0.96641273],
    ...21],
       [0.12634036, 0.70687794, 0.75467714, 0.11441949],
       [0.50397283, 0.35025146, 0.39421098, 0.45435234]])

    def test_mest(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = MEstimation()
&gt;       res = mod.decompose(X)

tests\test_matrix_factorization.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\decompy\matrix_factorization\mest.py:115: in decompose
    Sigma_end, Sigma_start, Bg = self._compute_scale_statistics(X, rank)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.mest.MEstimation object at 0x000001C990C96410&gt;
M = array([[0.70735595, 0.42514445, 0.33489327, 0.8183266 ],
       [0.99983977, 0.16107967, 0.65322436, 0.96641273],
    ...21],
       [0.12634036, 0.70687794, 0.75467714, 0.11441949],
       [0.50397283, 0.35025146, 0.39421098, 0.45435234]])
ini_rank = 1.0

    def _compute_scale_statistics(self, M: np.ndarray, ini_rank: int = None):
        """Compute scale statistics from input matrix M.
    
        Parameters
        ----------
        M : ndarray
            Input matrix.
        ini_rank : int, optional
            Initial rank for SVD computation. If None, set to ceil(M.shape[0]/10).
    
        Returns
        -------
        Sigmai : ndarray
            Scale matrix for input.
        Sigmaf : ndarray
            Scale matrix for error.
        basis_ini : ndarray
            Initial basis matrix.
    
        """
        # compute standard PCA
        mean_ls = np.mean(M, axis=0)
        U, _, VT = np.linalg.svd(M - mean_ls, full_matrices=False)
        if ini_rank is None:
            ini_rank = np.ceil(M.shape[0] / 10)
&gt;       U = U[:, :ini_rank]
E       TypeError: slice indices must be integers or None or have an __index__ method

src\decompy\matrix_factorization\mest.py:61: TypeError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_op" time="0.002"><failure message="ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 5)">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C47550&gt;
sample_matrix = array([[0.49208799, 0.51639391, 0.65716751, 0.98614187],
       [0.81591944, 0.65436367, 0.87606685, 0.13033936],
    ...83],
       [0.35449253, 0.04062768, 0.11266204, 0.94657641],
       [0.31836019, 0.49766249, 0.47509021, 0.46040667]])

    def test_op(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = OutlierPursuit()
&gt;       res = mod.decompose(X, rank=min(n, p))

tests\test_matrix_factorization.py:139: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\decompy\matrix_factorization\op.py:166: in decompose
    L_new, rank_L = self._iterate_L(GL, mu_temp / 2, rank_L + 1)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.op.OutlierPursuit object at 0x000001C990CE69E0&gt;
L = array([[0.246044  , 0.25819696, 0.32858375, 0.49307094],
       [0.40795972, 0.32718183, 0.43803343, 0.06516968],
    ...92],
       [0.17724627, 0.02031384, 0.05633102, 0.4732882 ],
       [0.15918009, 0.24883125, 0.23754511, 0.23020333]])
epsilon = 1.2886406611100993, starting_K = 5

    def _iterate_L(self, L, epsilon, starting_K):
        """Iterate matrix L towards rank K using SVD.
    
        Parameters
        ----------
        L : ndarray
            Input matrix to iterate on
        epsilon : float
            Threshold value for singular values
        starting_K : int
            Target rank K
    
        Returns
        -------
        output : ndarray
            Iterated L matrix
        rank_out : int
            Output rank of L
    
        """
        if not self.full_svd:
            pass
            # TODO: implement lansvd()
            # U, S, V = lansvd(L, starting_K, 'T', epsilon, self.increaseK)
            # rank_out = min(S.shape)
        else:
            U, S, Vh = np.linalg.svd(L)
            rank_out = 0
        S = np.where(S &gt; epsilon, S - epsilon, np.where(S &lt; -epsilon, S + epsilon, 0))
&gt;       output = U @ np.diag(S) @ Vh
E       ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 5)

src\decompy\matrix_factorization\op.py:63: ValueError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_pcp" time="0.016" /><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_reg1alm" time="0.040" /><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_svt" time="0.001"><failure message="ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 0)">self = &lt;tests.test_matrix_factorization.TestMatrixFactorization object at 0x000001C990C47A60&gt;
sample_matrix = array([[0.12522516, 0.14083561, 0.06435986, 0.77338476],
       [0.5793935 , 0.61058968, 0.29800155, 0.38092886],
    ...92],
       [0.87705951, 0.07783616, 0.70300904, 0.71547509],
       [0.26431663, 0.3335326 , 0.73560606, 0.6989254 ]])

    def test_svt(self, sample_matrix):
        X = sample_matrix
        n, p = X.shape
        mod = SingularValueThresholding()
&gt;       res = mod.decompose(X)

tests\test_matrix_factorization.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;decompy.matrix_factorization.svt.SingularValueThresholding object at 0x000001C992D22B30&gt;
M = array([[0.12522516, 0.14083561, 0.06435986, 0.77338476],
       [0.5793935 , 0.61058968, 0.29800155, 0.38092886],
    ...92],
       [0.87705951, 0.07783616, 0.70300904, 0.71547509],
       [0.26431663, 0.3335326 , 0.73560606, 0.6989254 ]])
lambdaval = 0.5, tau = 10000.0, delta = 0.9

    def decompose(
        self,
        M: np.ndarray,
        lambdaval: Union[float, None] = None,
        tau: Union[float, None] = None,
        delta: Union[float, None] = None,
    ):
        """Decompose a matrix M into low-rank (L) and sparse (S) components.
    
        Parameters
        ----------
        M : ndarray
            Input matrix to decompose
        lambdaval : float
            Regularization parameter for sparse component
        tau : float or None, optional
            Threshold for singular values, by default None
        delta : float or None, optional
            Step size for dual ascent, by default None
    
        Returns
        -------
        LSNResult
            Named tuple containing low-rank matrix L, sparse matrix S,
            noise matrix N, and convergence info
        """
        X = np.copy(M)
        n, p = X.shape
    
        # set options
        lambdaval = 1 / np.sqrt(min(n, p)) if lambdaval is None else lambdaval
        delta = 0.9 if delta is None else delta
        tau = 1e4 if tau is None else tau
    
        Y = np.zeros((n, p))  # lagrangian multiplier
        A = np.zeros((n, p))  # structure
        E = np.zeros((n, p))  # error
    
        niter = 0
        rankA = 0
        converged = False
    
        while not converged:
            niter += 1
            U, s, Vt = np.linalg.svd(Y, full_matrices=False)
            U = U[:, :rankA]
            Vt = Vt[:rankA, :]
    
&gt;           A = U @ np.diag(np.maximum(s - tau, 0)) @ Vt
E           ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 0)

src\decompy\matrix_factorization\svt.py:78: ValueError</failure></testcase><testcase classname="tests.test_matrix_factorization.TestMatrixFactorization" name="test_rsvddpd" time="0.053" /></testsuite></testsuites>